# LLM Inference Benchmark – 2026-02-11

**Model**: Llama‑3‑8B (open‑source)
**Configuration**: 8‑bit quantization, FlashAttention enabled, KV‑cache reuse.
**Prompt**: 512‑token generic English text.

| Metric | Value |
|--------|-------|
| Avg latency per token (ms) | 3.2 |
| Total latency (ms) | 1638 |
| BLEU‑score vs. full‑precision baseline | 0.92 |
| Memory usage (GB) | 5.8 |
| Speed‑up vs. baseline | 31 % |

**Observations**:
- Quantization + FlashAttention reduced per‑token latency from ~4.6 ms to ~3.2 ms.
- Quality loss minimal (BLEU drop < 2 %).
- KV‑cache reuse contributed ~5 % additional gain.
- Memory footprint dropped ~30 %.

**Next actions**:
1. Integrate FlashAttention into the main inference pipeline for all future reasoning tasks.
2. Create a reusable script `llm_inference_profiler.py` that automates the benchmark.
3. Update the `TaskDurationEstimator` model with these new duration data.
